# DNA-sequence-analysis
Fast progress in next generation sequencing of DNA has resulted in the availability of large DNA data
sets ready for analysis. However, DNA analysis has become the bottleneck in using these data sets,
as it requires powerful and scalable tools to perform the needed analysis. A typical analysis pipeline
consists of a number of steps, not all of which can readily scale on a distributed computing
infrastructure. In this assignment, you will create a framework that implements an in-memory
distributed version of the GATK DNA analysis pipeline using Apache Spark.  

The input data set to the pipeline is generated by sequencing a DNA sample in a sequencing machine and acquiring the
DNA sequencing data. This is done in a massively parallel fashion with millions of short DNA pieces
(called short reads) being sequenced at the same time. These reads are stored in large files of sizes in
the range of hundreds of giga bytes. The DNA is usually over-sampled, resulting in generating
multiple short reads for each segment of the DNA, typically with a coverage ranging from 30x to 80x,
depending on the experiment. One standard file format used today to store these reads is called the
FASTQ file format. Once the input data becomes available, the first step in the DNA analysis pipeline
is to align the sequenced reads to a human reference genome and reconstruct the sequenced
genome from the short read sequences, a process that is called DNA assembly. A well-known
program used to perform this step is the Burrows Wheel Aligner, typically using the BWA-MEM
algorithm. The results of this assembly step are stored in a file with the SAM file format.
After assembly, the short reads in the SAM file are sorted and duplicated reads are marked to avoid
using them in downstream analysis. This reduces the bias effect resulting from the way the DNA
sample is prepared and sequenced. A widely-used tool to perform this step is the Picard tool. The
output of this tool is a compressed file format called BAM. Subsequently, a number of steps are
performed to refine the read data and finally to identify the DNA mutations (or so-called variant
calling). GATK toolkit is a commonly-used toolkit for this type of analysis that contains the following
tools: Indel realignment, Base recalibration and the Haplotype caller. The output of this tool set is a
VCF (variant call format) file with the variations in the sequenced DNA. These widely-used tools
described here make part of the Broad best practices pipeline, widely used for variant calling both in
research and in the clinic. You will use the GATK pipeline for demonstrating the distribution
capabilities of Spark. If interested, you can find more info over GATK here
https://www.broadinstitute.org/gatk/
